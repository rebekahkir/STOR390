---
title: "Untitled"
output: html_document
date: "2024-02-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

---
title: "HW 3"
author: "Student Name"
date: "11/27/2023"
output: 
  html_document:
    number_sections: true
---

# 

In this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}

train_index <- sample(1:nrow(dat), 100)
train_data <- dat[train_index, ]
test_data <- dat[-train_index, ]

svmfit <- svm(y ~ ., data = train_data, kernel = "radial", gamma = 1, cost = 1, scale = FALSE)

plot(svmfit, train_data)

```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}

svmfit <- svm(y ~ ., data = train_data, kernel = "radial", gamma = 1, cost = 10000, scale = FALSE)

plot(svmfit, train_data)

```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

*The model is now at a higher risk for over fitting. Increasing the cost parameter increases the likelihood that the model will try so hard to fit noise in the training data that it becomes unable to be generalized to the testing data set. The model also has a higher computational complexity which increases the amount of time it takes to compute and can make it more difficult to interpret.*

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}

table(true=dat[-train_index,"y"], pred=predict(svmfit, newdata=dat[-train_index,]))
```


##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}

prop_class_2_train <- mean(train_data$y == 2)
prop_class_2_train


```

*There is a slight imbalance in the training/testing partition because class 2 makes up 27% of the training data and only 25% of the testing data.*

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}

set.seed(1)

cost_values <- c(0.1, 1, 10, 100, 1000)
gamma_values <- c(0.5, 1, 2, 3, 4)

parameter_grid <- expand.grid(cost = cost_values, gamma = gamma_values)

tune.out <- tune(svm, y ~ ., data = train_data, kernel = "radial", ranges = list(cost = cost_values, gamma = gamma_values))

tune.out


```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r, eval = FALSE}
table(true=dat[-train_index,"y"], pred=predict(tune.out$best.model, newdata=dat[-train_index,]))
```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

*In the first confusion matrix there were 18 observations that were incorrectly classified as class 1 and 5 observations incorrectly classified as class 2. In the second confusion matrix there were only 10 observations incorrectly classified as class 1 and 2 observations incorrectly classified as class 2. The new model reduced the number of misclassifications but we still need to cross-validate the model to make sure that it can be generalized to other data sets. *

# 
Let's turn now to decision trees.  

```{r}

library(kmed)
data(heart)
library(tree)


```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}

heart$cp_binary <- ifelse(heart$cp == "4", "yes", "no")
heart$cp_binary <- factor(heart$cp_binary, levels = c("no", "yes"))

str(heart$cp_binary)

```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)

train=sample(1:nrow(heart), 240)

tree.heart = tree(heart$cp_binary~.-class, heart, subset=train)
plot(tree.heart)
text(tree.heart, pretty=0)

```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}


```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}

cv.heart = cv.tree(tree.heart, FUN = prune.misclass)
cv.heart
```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

*In pruning the tree you have to sacrifice some accuracy in order to understand how the tree is splitting up factors. Being able to interpret the tree is a crucial part of making sure that the tree can be generalized to other datasets and also check for anything out of the ordinary that might cause bias. It's important to balance accuracy and interpretability because both of them are important.*

## 

Discuss the ways a decision tree could manifest algorithmic bias.  
*If the classes in the training data are imbalanced and one class is significantly more prevalent than others, the decision tree may prioritize accuracy on the larger class at the expense of the smaller class, which can lead to biased predictions. especially if the testing data is not representative of the training data. The complexity of the decision tree itself can also manifest bias because highly complex tree may be prone to overfitting. Trying to use a model that is tailored too much to the training data that it can't be general will cause innaccurate and often bias predictions.*
